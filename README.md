# evaluation-exploration

This is meant to be only for groundtruth citations evaluation, with the main purposes of:

- Tuning prompts
- Evaluating LLMs
- Possible post-/pre- processing of data before generating citations

This will be a very raw implementation to simply see the methodology and if it is useful to even have such a platform.

Given:

- Groundtruth:
  - Answer with citations
- To be evaluated:
  - List of documents
  - Answer
- Output:
  - Generated answer (by LLM) with citations
